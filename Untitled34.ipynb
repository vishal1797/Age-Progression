{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZld7pVxGnxL",
        "outputId": "96264a4d-ab85-49d9-95f8-8aec00a7e56e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting utils\n",
            "  Downloading utils-1.0.2.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: utils\n",
            "  Building wheel for utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13905 sha256=e8053af6288f08115dede40f2303ff582328b466c9947360217f332bc9f5e924\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/39/f5/9d0ca31dba85773ececf0a7f5469f18810e1c8a8ed9da28ca7\n",
            "Successfully built utils\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gB7-yziOGJeQ"
      },
      "outputs": [],
      "source": [
        "from utils import *\n",
        "#import consts\n",
        "\n",
        "import logging\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "#import imageio\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import l1_loss, mse_loss\n",
        "from torch.nn.functional import binary_cross_entropy_with_logits as bce_with_logits_loss\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        num_conv_layers = 6\n",
        "\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "\n",
        "        def add_conv(module_list, name, in_ch, out_ch, kernel, stride, padding, act_fn):\n",
        "            return module_list.add_module(\n",
        "                name,\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=in_ch,\n",
        "                        out_channels=out_ch,\n",
        "                        kernel_size=kernel,\n",
        "                        stride=stride,\n",
        "                    ),\n",
        "                    act_fn\n",
        "                )\n",
        "            )\n",
        "\n",
        "        add_conv(self.conv_layers, 'e_conv_1', in_ch=3, out_ch=64, kernel=5, stride=2, padding=2, act_fn=nn.ReLU())\n",
        "        add_conv(self.conv_layers, 'e_conv_2', in_ch=64, out_ch=128, kernel=5, stride=2, padding=2, act_fn=nn.ReLU())\n",
        "        add_conv(self.conv_layers, 'e_conv_3', in_ch=128, out_ch=256, kernel=5, stride=2, padding=2, act_fn=nn.ReLU())\n",
        "        add_conv(self.conv_layers, 'e_conv_4', in_ch=256, out_ch=512, kernel=5, stride=2, padding=2, act_fn=nn.ReLU())\n",
        "        add_conv(self.conv_layers, 'e_conv_5', in_ch=512, out_ch=1024, kernel=5, stride=2, padding=2, act_fn=nn.ReLU())\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    ('e_fc_1', nn.Linear(in_features=1024, out_features=consts.NUM_Z_CHANNELS)),\n",
        "                    ('tanh_1', nn.Tanh())  # normalize to [-1, 1] range\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, face):\n",
        "        out = face\n",
        "        for conv_layer in self.conv_layers:\n",
        "            #print(\"H\")\n",
        "            out = conv_layer(out)\n",
        "            #print(out.shape)\n",
        "            #print(\"W\")\n",
        "        out = out.flatten(1, -1)\n",
        "        out = self.fc_layer(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "2-UA6XBtGLSd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscriminatorZ(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiscriminatorZ, self).__init__()\n",
        "        dims = (consts.NUM_Z_CHANNELS, consts.NUM_ENCODER_CHANNELS, consts.NUM_ENCODER_CHANNELS // 2,\n",
        "                consts.NUM_ENCODER_CHANNELS // 4)\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i, (in_dim, out_dim) in enumerate(zip(dims[:-1], dims[1:]), 1):\n",
        "            self.layers.add_module(\n",
        "                'dz_fc_%d' % i,\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(in_dim, out_dim),\n",
        "                    nn.BatchNorm1d(out_dim),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.layers.add_module(\n",
        "            'dz_fc_%d' % (i + 1),\n",
        "            nn.Sequential(\n",
        "                nn.Linear(out_dim, 1),\n",
        "                # nn.Sigmoid()  # commented out because logits are needed\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = z\n",
        "        for layer in self.layers:\n",
        "            out = layer(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "RAJjckq9G2ro"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DiscriminatorImg(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiscriminatorImg, self).__init__()\n",
        "        in_dims = (3, 16 + consts.LABEL_LEN_EXPANDED, 32, 64)\n",
        "        out_dims = (16, 32, 64, 128)\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        for i, (in_dim, out_dim) in enumerate(zip(in_dims, out_dims), 1):\n",
        "            self.conv_layers.add_module(\n",
        "                'dimg_conv_%d' % i,\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(in_dim, out_dim, kernel_size=2, stride=2),\n",
        "                    nn.BatchNorm2d(out_dim),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.fc_layers.add_module(\n",
        "            'dimg_fc_1',\n",
        "            nn.Sequential(\n",
        "                nn.Linear(128 * 8 * 8, 1024),\n",
        "                nn.LeakyReLU()\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.fc_layers.add_module(\n",
        "            'dimg_fc_2',\n",
        "            nn.Sequential(\n",
        "                nn.Linear(1024, 1),\n",
        "                # nn.Sigmoid()  # commented out because logits are needed\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, imgs, labels, device):\n",
        "        out = imgs\n",
        "\n",
        "        # run convs\n",
        "        for i, conv_layer in enumerate(self.conv_layers, 1):\n",
        "            # print(out.shape)\n",
        "            # print(conv_layer)\n",
        "            out = conv_layer(out)\n",
        "            if i == 1:\n",
        "                # concat labels after first conv\n",
        "                labels_tensor = torch.zeros(torch.Size((out.size(0), labels.size(1), out.size(2), out.size(3))), device=device)\n",
        "                for img_idx in range(out.size(0)):\n",
        "                    for label in range(labels.size(1)):\n",
        "                        labels_tensor[img_idx, label, :, :] = labels[img_idx, label]  # fill a square\n",
        "                out = torch.cat((out, labels_tensor), 1)\n",
        "\n",
        "        # run fcs\n",
        "        out = out.flatten(1, -1)\n",
        "        for fc_layer in self.fc_layers:\n",
        "            # print(out.shape)\n",
        "            # print(fc_layer)\n",
        "\n",
        "            out = fc_layer(out)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "D2dt1F-UG6sZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        num_deconv_layers = 5\n",
        "        mini_size = 4\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                consts.NUM_Z_CHANNELS + consts.LABEL_LEN_EXPANDED,\n",
        "                consts.NUM_GEN_CHANNELS * mini_size ** 2\n",
        "            ),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # need to reshape now to ?,1024,8,8\n",
        "\n",
        "        self.deconv_layers = nn.ModuleList()\n",
        "\n",
        "        def add_deconv(name, in_dims, out_dims, kernel, stride, actf):\n",
        "            self.deconv_layers.add_module(\n",
        "                name,\n",
        "                nn.Sequential(\n",
        "                    easy_deconv(\n",
        "                        in_dims=in_dims,\n",
        "                        out_dims=out_dims,\n",
        "                        kernel=kernel,\n",
        "                        stride=stride,\n",
        "                    ),\n",
        "                    actf\n",
        "                )\n",
        "            )\n",
        "\n",
        "        add_deconv('g_deconv_1', in_dims=(1024, 4, 4), out_dims=(512, 8, 8), kernel=5, stride=2, actf=nn.ReLU())\n",
        "        add_deconv('g_deconv_2', in_dims=(512, 8, 8), out_dims=(256, 16, 16), kernel=5, stride=2, actf=nn.ReLU())\n",
        "        add_deconv('g_deconv_3', in_dims=(256, 16, 16), out_dims=(128, 32, 32), kernel=5, stride=2, actf=nn.ReLU())\n",
        "        add_deconv('g_deconv_4', in_dims=(128, 32, 32), out_dims=(64, 64, 64), kernel=5, stride=2, actf=nn.ReLU())\n",
        "        add_deconv('g_deconv_5', in_dims=(64, 64, 64), out_dims=(32, 128, 128), kernel=5, stride=2, actf=nn.ReLU())\n",
        "        add_deconv('g_deconv_6', in_dims=(32, 128, 128), out_dims=(16, 128, 128), kernel=5, stride=1, actf=nn.ReLU())\n",
        "        add_deconv('g_deconv_7', in_dims=(16, 128, 128), out_dims=(3, 128, 128), kernel=1, stride=1, actf=nn.Tanh())\n",
        "\n",
        "    def _decompress(self, x):\n",
        "        return x.view(x.size(0), 1024, 4, 4)  # TODO - replace hardcoded\n",
        "\n",
        "    def forward(self, z, age=None, gender=None):\n",
        "        out = z\n",
        "        if age is not None and gender is not None:\n",
        "            label = Label(age, gender).to_tensor() \\\n",
        "                if (isinstance(age, int) and isinstance(gender, int)) \\\n",
        "                else torch.cat((age, gender), 1)\n",
        "            out = torch.cat((out, label), 1)  # z_l\n",
        "        #print(out.shape)\n",
        "        out = self.fc(out)\n",
        "        #print(out.shape)\n",
        "        out = self._decompress(out)\n",
        "        #print(out.shape)\n",
        "        for i, deconv_layer in enumerate(self.deconv_layers, 1):\n",
        "            out = deconv_layer(out)\n",
        "            #print(out.shape)\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rn4P8QUxG-mv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(object):\n",
        "    def __init__(self):\n",
        "        self.E = Encoder()\n",
        "        self.Dz = DiscriminatorZ()\n",
        "        self.Dimg = DiscriminatorImg()\n",
        "        self.G = Generator()\n",
        "\n",
        "        self.eg_optimizer = Adam(list(self.E.parameters()) + list(self.G.parameters()))\n",
        "        self.dz_optimizer = Adam(self.Dz.parameters())\n",
        "        self.di_optimizer = Adam(self.Dimg.parameters())\n",
        "\n",
        "        self.device = None\n",
        "        self.cpu()  # initial, can later move to cuda\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        self.test_single(*args, **kwargs)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return os.linesep.join([repr(subnet) for subnet in (self.E, self.Dz, self.G)])\n",
        "\n",
        "    def morph(self, image_tensors, ages, genders, length, target):\n",
        "\n",
        "        self.eval()\n",
        "\n",
        "        original_vectors = [None, None]\n",
        "        for i in range(2):\n",
        "            z = self.E(image_tensors[i].unsqueeze(0))\n",
        "            l = Label(ages[i], genders[i]).to_tensor(normalize=True).unsqueeze(0).to(device=z.device)\n",
        "            z_l = torch.cat((z, l), 1)\n",
        "            original_vectors[i] = z_l\n",
        "\n",
        "        z_vectors = torch.zeros((length + 1, z_l.size(1)), dtype=z_l.dtype)\n",
        "        for i in range(length + 1):\n",
        "            z_vectors[i, :] = original_vectors[0].mul(length - i).div(length) + original_vectors[1].mul(i).div(length)\n",
        "\n",
        "        generated = self.G(z_vectors)\n",
        "        dest = os.path.join(target, 'morph.png')\n",
        "        save_image_normalized(tensor=generated, filename=dest, nrow=generated.size(0))\n",
        "        print_timestamp(\"Saved test result to \" + dest)\n",
        "        return dest\n",
        "\n",
        "    def kids(self, image_tensors, length, target):\n",
        "\n",
        "        self.eval()\n",
        "\n",
        "        original_vectors = [None, None]\n",
        "        for i in range(2):\n",
        "            z = self.E(image_tensors[i].unsqueeze(0)).squeeze(0)\n",
        "            original_vectors[i] = z\n",
        "\n",
        "        z_vectors = torch.zeros((length, consts.NUM_Z_CHANNELS), dtype=z.dtype)\n",
        "        z_l_vectors = torch.zeros((length, consts.NUM_Z_CHANNELS + consts.LABEL_LEN_EXPANDED), dtype=z.dtype)\n",
        "        for i in range(length):\n",
        "            for j in range(consts.NUM_Z_CHANNELS):\n",
        "                r = random.random()\n",
        "                z_vectors[i][j] = original_vectors[0][j].mul(r) + original_vectors[1][j].mul(1 - r)\n",
        "\n",
        "            fake_age = 0\n",
        "            fake_gender = random.choice([consts.MALE, consts.FEMALE])\n",
        "            l = Label(fake_age, fake_gender).to_tensor(normalize=True).to(device=z.device)\n",
        "            z_l = torch.cat((z_vectors[i], l), 0)\n",
        "            z_l_vectors[i, :] = z_l\n",
        "\n",
        "        generated = self.G(z_l_vectors)\n",
        "        dest = os.path.join(target, 'kids.png')\n",
        "        save_image_normalized(tensor=generated, filename=dest, nrow=generated.size(0))\n",
        "        print_timestamp(\"Saved test result to \" + dest)\n",
        "        return dest\n",
        "\n",
        "\n",
        "    def test_single(self, image_tensor, age, gender, target, watermark):\n",
        "\n",
        "        self.eval()\n",
        "        batch = image_tensor.repeat(consts.NUM_AGES, 1, 1, 1).to(device=self.device)  # N x D x H x W\n",
        "        z = self.E(batch)  # N x Z\n",
        "\n",
        "        gender_tensor = -torch.ones(consts.NUM_GENDERS)\n",
        "        gender_tensor[int(gender)] *= -1\n",
        "        gender_tensor = gender_tensor.repeat(consts.NUM_AGES, consts.NUM_AGES // consts.NUM_GENDERS)  # apply gender on all images\n",
        "\n",
        "        age_tensor = -torch.ones(consts.NUM_AGES, consts.NUM_AGES)\n",
        "        for i in range(consts.NUM_AGES):\n",
        "            age_tensor[i][i] *= -1  # apply the i'th age group on the i'th image\n",
        "\n",
        "        l = torch.cat((age_tensor, gender_tensor), 1).to(self.device)\n",
        "        z_l = torch.cat((z, l), 1)\n",
        "\n",
        "        generated = self.G(z_l)\n",
        "\n",
        "        if watermark:\n",
        "            image_tensor = image_tensor.permute(1, 2, 0)\n",
        "            image_tensor = 255 * one_sided(image_tensor.numpy())\n",
        "            image_tensor = np.ascontiguousarray(image_tensor, dtype=np.uint8)\n",
        "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "            bottomLeftCornerOfText = (2, 25)\n",
        "            fontScale = 0.5\n",
        "            fontColor = (0, 128, 0)  # dark green, should be visible on most skin colors\n",
        "            lineType = 2\n",
        "            cv2.putText(\n",
        "                image_tensor,\n",
        "                '{}, {}'.format([\"Male\", \"Female\"][gender], age),\n",
        "                bottomLeftCornerOfText,\n",
        "                font,\n",
        "                fontScale,\n",
        "                fontColor,\n",
        "                lineType,\n",
        "\n",
        "            )\n",
        "            image_tensor = two_sided(torch.from_numpy(image_tensor / 255.0)).float().permute(2, 0, 1)\n",
        "\n",
        "        joined = torch.cat((image_tensor.unsqueeze(0), generated), 0)\n",
        "\n",
        "        joined = nn.ConstantPad2d(padding=4, value=-1)(joined)\n",
        "        for img_idx in (0, Label.age_transform(age) + 1):\n",
        "            for elem_idx in (0, 1, 2, 3, -4, -3, -2, -1):\n",
        "                joined[img_idx, :, elem_idx, :] = 1  # color border white\n",
        "                joined[img_idx, :, :, elem_idx] = 1  # color border white\n",
        "\n",
        "        dest = os.path.join(target, 'out_{0}_{1}.png'.format(gender, age))\n",
        "\n",
        "        #show and save the input and latest age\n",
        "        s_head_tail = False\n",
        "        if s_head_tail: joined = joined[::len(joined)-1] #first and last item\n",
        "\n",
        "\n",
        "        save_image_normalized(tensor=joined, filename=dest, nrow=joined.size(0))\n",
        "        print_timestamp(\"Saved test result to \" + dest)\n",
        "        return dest\n",
        "\n",
        "    def teach(\n",
        "            self,\n",
        "            utkface_path,\n",
        "            batch_size=64,\n",
        "            epochs=1,\n",
        "            weight_decay=1e-5,\n",
        "            lr=2e-4,\n",
        "            should_plot=False,\n",
        "            betas=(0.9, 0.999),\n",
        "            valid_size=None,\n",
        "            where_to_save=None,\n",
        "            models_saving='always',\n",
        "    ):\n",
        "        where_to_save = where_to_save or default_where_to_save()\n",
        "        dataset = get_utkface_dataset(utkface_path)\n",
        "        valid_size = valid_size or batch_size\n",
        "        valid_dataset, train_dataset = torch.utils.data.random_split(dataset, (valid_size, len(dataset) - valid_size))\n",
        "\n",
        "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "        idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
        "\n",
        "        input_output_loss = l1_loss\n",
        "        nrow = round((2 * batch_size)**0.5)\n",
        "\n",
        "        # save_image_normalized(tensor=validate_images, filename=where_to_save+\"/base.png\")\n",
        "\n",
        "        for optimizer in (self.eg_optimizer, self.dz_optimizer, self.di_optimizer):\n",
        "            for param in ('weight_decay', 'betas', 'lr'):\n",
        "                val = locals()[param]\n",
        "                if val is not None:\n",
        "                    optimizer.param_groups[0][param] = val\n",
        "\n",
        "        loss_tracker = LossTracker(plot=should_plot)\n",
        "        where_to_save_epoch = \"\"\n",
        "        save_count = 0\n",
        "        paths_for_gif = []\n",
        "\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            where_to_save_epoch = os.path.join(where_to_save, \"epoch\" + str(epoch))\n",
        "            try:\n",
        "                if not os.path.exists(where_to_save_epoch):\n",
        "                    os.makedirs(where_to_save_epoch)\n",
        "                paths_for_gif.append(where_to_save_epoch)\n",
        "                losses = defaultdict(lambda: [])\n",
        "                self.train()  # move to train mode\n",
        "                for i, (images, labels) in enumerate(train_loader, 1):\n",
        "\n",
        "                    images = images.to(device=self.device)\n",
        "                    labels = torch.stack([str_to_tensor(idx_to_class[l], normalize=True) for l in list(labels.numpy())])  # todo - can remove list() ?\n",
        "                    labels = labels.to(device=self.device)\n",
        "                    # print (\"DEBUG: iteration: \"+str(i)+\" images shape: \"+str(images.shape))\n",
        "                    z = self.E(images)\n",
        "\n",
        "                    # Input\\Output Loss\n",
        "                    z_l = torch.cat((z, labels), 1)\n",
        "                    generated = self.G(z_l)\n",
        "                    eg_loss = input_output_loss(generated, images)\n",
        "                    losses['eg'].append(eg_loss.item())\n",
        "\n",
        "                    # Total Variance Regularization Loss\n",
        "                    reg = l1_loss(generated[:, :, :, :-1], generated[:, :, :, 1:]) + l1_loss(generated[:, :, :-1, :], generated[:, :, 1:, :])\n",
        "\n",
        "                    # reg = (\n",
        "                    #        torch.sum(torch.abs(generated[:, :, :, :-1] - generated[:, :, :, 1:])) +\n",
        "                    #        torch.sum(torch.abs(generated[:, :, :-1, :] - generated[:, :, 1:, :]))\n",
        "                    # ) / float(generated.size(0))\n",
        "                    reg_loss = 0 * reg\n",
        "                    reg_loss.to(self.device)\n",
        "                    losses['reg'].append(reg_loss.item())\n",
        "\n",
        "                    # DiscriminatorZ Loss\n",
        "                    z_prior = two_sided(torch.rand_like(z, device=self.device))  # [-1 : 1]\n",
        "                    d_z_prior = self.Dz(z_prior)\n",
        "                    d_z = self.Dz(z)\n",
        "                    dz_loss_prior = bce_with_logits_loss(d_z_prior, torch.ones_like(d_z_prior))\n",
        "                    dz_loss = bce_with_logits_loss(d_z, torch.zeros_like(d_z))\n",
        "                    dz_loss_tot = (dz_loss + dz_loss_prior)\n",
        "                    losses['dz'].append(dz_loss_tot.item())\n",
        "\n",
        "                    # Encoder\\DiscriminatorZ Loss\n",
        "                    ez_loss = 0.0001 * bce_with_logits_loss(d_z, torch.ones_like(d_z))\n",
        "                    ez_loss.to(self.device)\n",
        "                    losses['ez'].append(ez_loss.item())\n",
        "\n",
        "                    # DiscriminatorImg Loss\n",
        "                    d_i_input = self.Dimg(images, labels, self.device)\n",
        "                    d_i_output = self.Dimg(generated, labels, self.device)\n",
        "\n",
        "                    di_input_loss = bce_with_logits_loss(d_i_input, torch.ones_like(d_i_input))\n",
        "                    di_output_loss = bce_with_logits_loss(d_i_output, torch.zeros_like(d_i_output))\n",
        "                    di_loss_tot = (di_input_loss + di_output_loss)\n",
        "                    losses['di'].append(di_loss_tot.item())\n",
        "\n",
        "                    # Generator\\DiscriminatorImg Loss\n",
        "                    dg_loss = 0.0001 * bce_with_logits_loss(d_i_output, torch.ones_like(d_i_output))\n",
        "                    losses['dg'].append(dg_loss.item())\n",
        "\n",
        "                    # this loss is only for debugging\n",
        "                    uni_diff_loss = (uni_loss(z.cpu().detach()) - uni_loss(z_prior.cpu().detach())) / batch_size\n",
        "                    # losses['uni_diff'].append(uni_diff_loss)\n",
        "\n",
        "\n",
        "                    # Start back propagation\n",
        "\n",
        "                    # Back prop on Encoder\\Generator\n",
        "                    self.eg_optimizer.zero_grad()\n",
        "                    loss = eg_loss + reg_loss + ez_loss + dg_loss\n",
        "                    loss.backward(retain_graph=True)\n",
        "                    self.eg_optimizer.step()\n",
        "\n",
        "                    # Back prop on DiscriminatorZ\n",
        "                    self.dz_optimizer.zero_grad()\n",
        "                    dz_loss_tot.backward(retain_graph=True)\n",
        "                    self.dz_optimizer.step()\n",
        "\n",
        "                    # Back prop on DiscriminatorImg\n",
        "                    self.di_optimizer.zero_grad()\n",
        "                    di_loss_tot.backward()\n",
        "                    self.di_optimizer.step()\n",
        "\n",
        "                    now = datetime.datetime.now()\n",
        "\n",
        "                logging.info('[{h}:{m}[Epoch {e}] Loss: {t}'.format(h=now.hour, m=now.minute, e=epoch, t=loss.item()))\n",
        "                print_timestamp(\"[Epoch {epoch:d}] Loss: {loss.item():f}\")\n",
        "                to_save_models = models_saving in ('always', 'tail')\n",
        "                cp_path = self.save(where_to_save_epoch, to_save_models=to_save_models)\n",
        "                if models_saving == 'tail':\n",
        "                    prev_folder = os.path.join(where_to_save, \"epoch\" + str(epoch - 1))\n",
        "                    remove_trained(prev_folder)\n",
        "                loss_tracker.save(os.path.join(cp_path, 'losses.png'))\n",
        "\n",
        "                with torch.no_grad():  # validation\n",
        "                    self.eval()  # move to eval mode\n",
        "\n",
        "                    for ii, (images, labels) in enumerate(valid_loader, 1):\n",
        "                        images = images.to(self.device)\n",
        "                        labels = torch.stack([str_to_tensor(idx_to_class[l], normalize=True) for l in list(labels.numpy())])\n",
        "                        labels = labels.to(self.device)\n",
        "                        validate_labels = labels.to(self.device)\n",
        "\n",
        "                        z = self.E(images)\n",
        "                        z_l = torch.cat((z, validate_labels), 1)\n",
        "                        generated = self.G(z_l)\n",
        "\n",
        "                        loss = input_output_loss(images, generated)\n",
        "\n",
        "                        joined = merge_images(images, generated)  # torch.cat((generated, images), 0)\n",
        "\n",
        "                        file_name = os.path.join(where_to_save_epoch, 'validation.png')\n",
        "                        save_image_normalized(tensor=joined, filename=file_name, nrow=nrow)\n",
        "\n",
        "                        losses['valid'].append(loss.item())\n",
        "                        break\n",
        "\n",
        "\n",
        "                loss_tracker.append_many(**{k: mean(v) for k, v in losses.items()})\n",
        "                loss_tracker.plot()\n",
        "\n",
        "                logging.info('[{h}:{m}[Epoch {e}] Loss: {l}'.format(h=now.hour, m=now.minute, e=epoch, l=repr(loss_tracker)))\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print_timestamp(\"{br}CTRL+C detected, saving model{br}\".format(br=os.linesep))\n",
        "                if models_saving != 'never':\n",
        "                    cp_path = self.save(where_to_save_epoch, to_save_models=True)\n",
        "                if models_saving == 'tail':\n",
        "                    prev_folder = os.path.join(where_to_save, \"epoch\" + str(epoch - 1))\n",
        "                    remove_trained(prev_folder)\n",
        "                loss_tracker.save(os.path.join(cp_path, 'losses.png'))\n",
        "                raise\n",
        "\n",
        "        if models_saving == 'last':\n",
        "            cp_path = self.save(where_to_save_epoch, to_save_models=True)\n",
        "        loss_tracker.plot()\n",
        "\n",
        "    def _mass_fn(self, fn_name, *args, **kwargs):\n",
        "        \"\"\"Apply a function to all possible Net's components.\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        for class_attr in dir(self):\n",
        "            if not class_attr.startswith('_'):  # ignore private members, for example self.__class__\n",
        "                class_attr = getattr(self, class_attr)\n",
        "                if hasattr(class_attr, fn_name):\n",
        "                    fn = getattr(class_attr, fn_name)\n",
        "                    fn(*args, **kwargs)\n",
        "\n",
        "    def to(self, device):\n",
        "        self._mass_fn('to', device=device)\n",
        "\n",
        "    def cpu(self):\n",
        "        self._mass_fn('cpu')\n",
        "        self.device = torch.device('cpu')\n",
        "\n",
        "    def cuda(self):\n",
        "        self._mass_fn('cuda')\n",
        "        self.device = torch.device('cuda')\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"Move Net to evaluation mode.\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        self._mass_fn('eval')\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Move Net to training mode.\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        self._mass_fn('train')\n",
        "\n",
        "    def save(self, path, to_save_models=True):\n",
        "        \"\"\"Save all state dicts of Net's components.\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(path):\n",
        "            os.mkdir(path)\n",
        "        # path = os.path.join(path, datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
        "        if not os.path.isdir(path):\n",
        "            os.mkdir(path)\n",
        "\n",
        "        saved = []\n",
        "        if to_save_models:\n",
        "            for class_attr_name in dir(self):\n",
        "                if not class_attr_name.startswith('_'):\n",
        "                    class_attr = getattr(self, class_attr_name)\n",
        "                    if hasattr(class_attr, 'state_dict'):\n",
        "                        state_dict = class_attr.state_dict\n",
        "                        fname = os.path.join(path, consts.TRAINED_MODEL_FORMAT.format(class_attr_name))\n",
        "                        torch.save(state_dict, fname)\n",
        "                        saved.append(class_attr_name)\n",
        "\n",
        "        if saved:\n",
        "            print_timestamp(\"Saved {} to {}\".format(', '.join(saved), path))\n",
        "        elif to_save_models:\n",
        "            raise FileNotFoundError(\"Nothing was saved to {}\".format(path))\n",
        "        return path\n",
        "\n",
        "    def load(self, path, slim=True):\n",
        "        \"\"\"Load all state dicts of Net's components.\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        loaded = []\n",
        "        for class_attr_name in dir(self):\n",
        "            if (not class_attr_name.startswith('_')) and ((not slim) or (class_attr_name in ('E', 'G'))):\n",
        "                class_attr = getattr(self, class_attr_name)\n",
        "                fname = os.path.join(path, consts.TRAINED_MODEL_FORMAT.format(class_attr_name))\n",
        "                if hasattr(class_attr, 'load_state_dict') and os.path.exists(fname):\n",
        "                    class_attr.load_state_dict(torch.load(fname)())\n",
        "                    loaded.append(class_attr_name)\n",
        "        if loaded:\n",
        "            print_timestamp(\"Loaded {} from {}\".format(', '.join(loaded), path))\n",
        "        else:\n",
        "            raise FileNotFoundError(\"Nothing was loaded from {}\".format(path))\n",
        "\n",
        "\n",
        "def create_list_of_img_paths(pattern, start, step):\n",
        "    result = []\n",
        "    fname = pattern.format(start)\n",
        "    while os.path.isfile(fname):\n",
        "        result.append(fname)\n",
        "        start += step\n",
        "        fname = pattern.format(start)\n",
        "    return result\n",
        "\n",
        "def create_gif(img_paths, dst, start, step):\n",
        "    BLACK = (255, 255, 255)\n",
        "    WHITE = (255, 255, 255)\n",
        "    MAX_LEN = 1024\n",
        "    frames = []\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    corner = (2, 25)\n",
        "    fontScale = 0.5\n",
        "    fontColor = BLACK\n",
        "    lineType = 2\n",
        "    for path in img_paths:\n",
        "        image = cv2.imread(path)\n",
        "        height, width = image.shape[:2]\n",
        "        current_max = max(height, width)\n",
        "        if current_max > MAX_LEN:\n",
        "            height = int(height / current_max * MAX_LEN)\n",
        "            width = int(width / current_max * MAX_LEN)\n",
        "            image = cv2.resize(image, (width, height), interpolation=cv2.INTER_CUBIC)\n",
        "        image = cv2.copyMakeBorder(image, 50, 0, 0, 0, cv2.BORDER_CONSTANT, WHITE)\n",
        "        cv2.putText(image, 'Epoch: ' + str(start), corner, font, fontScale, fontColor, lineType)\n",
        "        image = image[..., ::-1]\n",
        "        frames.append(image)\n",
        "        start += step\n",
        "    imageio.mimsave(dst, frames, 'GIF', duration=0.5)"
      ],
      "metadata": {
        "id": "kdPLud9wHC2V"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4K4Iy16vHNjf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}